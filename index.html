---
layout: default
title: Tobias Hinz | ML Research Engineer @ Adobe Research
description: I work with generative neural networks to model and understand the world around us.
---

<!-- Overview -->
<section id="overview" class="wrapper style1">
	<div class="container 75%">
		<ul class="row 200%">
			<ul class="6u 12u$(medium)">
				<header class="major">
					<h2>TOBIAS HINZ</h2>
					<div class="image rounded" style="margin-bottom: 2em;"><img src="images/photo.jpg" width="200"
							alt="Avatar" /></div>
					<p style="line-height:1.0">Research Scientist</p>
					<p style="line-height:1.0">Meta Superintelligence Labs</p>
					<p style="line-height:1.8">Meta</p>
				</header>
			</ul>
			<div class="6u$ 12u$(medium)" align="justify">
				<p>I am a Research Scientist at <a href="https://www.meta.com/" target="_blank">Meta</a> Superintelligence Labs,
					    where I work on generative media models for multimodal data focusing on image, audio, and video generation.
						Previously I was the tech lead for core foundational models at <a
						href="https://www.adobe.com/sensei/generative-ai/firefly.html" target="_blank">Adobe
						Firefly</a>, responsible for developing the next generation of Generative AI models, and
						a Research Engineer at <a href="https://research.adobe.com/" target="_blank">Adobe
						Research</a> where I started the engineering efforts of Adobe Firefly.
				</p>
				<p>I obtained my PhD at the <a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/"
						target="_blank">Knowledge Technology</a> group at the <a href="https://www.uni-hamburg.de/"
						target="_blank">University of Hamburg</a> (Germany).
					Before that, I completed a research oriented Master's degree in <a
						href="https://www.inf.uni-hamburg.de/studies/master/ias.html" target="_blank">Intelligent
						Adaptive Systems</a> at the University of Hamburg.
					I received a Bachelor's degree in <a href="https://www.wim.uni-mannheim.de/en/business-informatics/"
						target="_blank">Business Informatics</a> at the <a href="https://www.uni-mannheim.de/"
						target="_blank">University of Mannheim</a>, during which I also studied at the <a
						href="http://nus.edu.sg/" target="_blank">National University of Singapore</a> for one semester.
				</p>
			</div>
		</ul>
https://www.instagram.com/p/DQ7WH5xkoXB/
		<h2 style="text-align: center">News</h2>
		<ul></ul>
		<li>06/2025: Check out the first model release I contributed to at Meta: <a
				href="https://www.instagram.com/p/DQ7WH5xkoXB/"
				target="_blank">lip-syncing</a>.</li>
		<li>06/2025: I started a new role as Research Scientist at Meta GenAI Research in Menlo Park.</li>
		<li>03/2025: Our <a href="https://shotadapter.github.io/" target="_blank">ShotAdapter paper</a> on text-to-multi-shot video generation with diffusion models got accepted to CVPR 2025.</li>
		<li>10/2024: I attended <a href="https://www.adobe.com/max.html" target="_blank">Adobe MAX</a> where Adobe
			announced the public beta of our <a
				href="https://blog.adobe.com/en/publish/2024/10/14/generate-video-beta-on-firefly-web-app"
				target="_blank">text-to-video model</a> as well as <a
				href="https://blog.adobe.com/en/publish/2024/10/14/generative-extend-in-premiere-pro"
				target="_blank">GenExtend</a> (powered by our video model) integrated in Premiere Pro.</li>
		<li>09/2024: A first sneak peek of our foundational <a
				href="https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon"
				target="_blank">text-to-video model</a> at Adobe Firefly that we've been
			working on for the past year.</li>
		<li>06/2024: I attended CVPR 2024 in Seattle. Lots of great papers and conversations.</li>
		<li>03/2024: Our papers on <a
				href="https://openaccess.thecvf.com/content/CVPR2024/html/Ham_Personalized_Residuals_for_Concept-Driven_Text-to-Image_Generation_CVPR_2024_paper.html"
				target="_blank">Personalized T2I</a> and <a
				href="https://openaccess.thecvf.com/content/CVPR2024/html/Li_SNED_Superposition_Network_Architecture_Search_for_Efficient_Video_Diffusion_Model_CVPR_2024_paper.html"
				target="_blank">Efficient Video Diffusion</a> got accepted to CVPR 2024.</li>
		<li>11/2023: I have started leading the efforts on developing foundational text-to-video models for Adobe
			Firefly.</li>
		<li>10/2023: Our second generation of <a href="https://helpx.adobe.com/firefly/using/whats-new/2024.html"
				target="_blank">Firefly</a> models was announced today at the Adobe MAX conference.</li>
		<li>08/2023: Our <a href="https://mcm-diffusion.github.io/" target="_blank">paper on diffusion model
				control</a> was presented at SIGGRAPH.</li>
		<li>07/2023: I received the Adobe Tech Excellence Award, an annual recognition of outstanding technical
			talent for starting the engineering efforts of Adobe's Firefly models, writing more than 80% of the
			initial code base and serving as tech lead for foundational models.</li>
		</ul>
	</div>
</section>

<!-- Shipped Features -->
<section id="blog" class="wrapper style1 special">
	<div class="container">
		<h2 style="text-align: center">Features I Worked On</h2>
		<style>
			.features-grid {
				display: grid;
				grid-template-columns: repeat(2, 1fr);
				gap: 2rem;
				max-width: 1200px;
				margin: 0 auto;
				padding: 1rem;
			}

			.feature-card {
				display: flex;
				align-items: center;
				gap: 1.5rem;
				padding: 1.5rem;
				background: rgba(255, 255, 255, 0.05);
				border-radius: 8px;
			}

			.feature-image {
				flex-shrink: 0;
				width: 150px;
				height: 150px;
				object-fit: cover;
				border-radius: 4px;
			}

			.feature-content {
				flex-grow: 1;
			}

			.feature-content a {
				font-size: 1.1rem;
				font-weight: bold;
				text-decoration: none;
			}

			.feature-content .date {
				font-weight: bold;
				margin: 0.5rem 0;
			}

			.feature-content .description {
				font-size: 0.9rem;
				line-height: 1.4;
			}

			@media (max-width: 768px) {
				.features-grid {
					grid-template-columns: 1fr;
				}

				.feature-card {
					flex-direction: column;
					text-align: center;
				}

				.feature-image {
					width: 120px;
					height: 120px;
				}
			}
		</style>

		<div class="features-grid">
			<div class="feature-card">
				<img class="feature-image" src="images/features/adobe_firefly_video.jpg" alt="Adobe Firefly Video">
				<div class="feature-content">
					<a href="https://blog.adobe.com/en/publish/2024/09/11/bringing-gen-ai-to-video-adobe-firefly-video-model-coming-soon" target="_blank">Adobe Firefly Video Model</a>
					<div class="date">October 2024</div>
					<div class="description">Oversaw technical development of the video model including training of the models and data preparation.</div>
				</div>
			</div>

			<div class="feature-card">
				<img class="feature-image" src="images/features/adobe_genextend.jpg" alt="Adobe GenExtend">
				<div class="feature-content">
					<a href="https://blog.adobe.com/en/publish/2024/10/14/generative-extend-in-premiere-pro" target="_blank">Adobe Firefly Video GenExtend</a>
					<div class="date">October 2024</div>
					<div class="description">Consulted on training and development approaches.</div>
				</div>
			</div>

			<div class="feature-card">
				<img class="feature-image" src="images/features/adobe_firefly.jpg" alt="Adobe Firefly v2">
				<div class="feature-content">
					<a href="https://news.adobe.com/news/news-details/2023/adobe-releases-next-generation-of-firefly-models" target="_blank">Adobe Firefly Image Model v2</a>
					<div class="date">October 2023</div>
					<div class="description">Developed and trained parts of the model; oversaw overall technical development of the core diffusion model.</div>
				</div>
			</div>

			<div class="feature-card">
				<img class="feature-image" src="images/features/adobe_firefly.jpg" alt="Adobe Firefly">
				<div class="feature-content">
					<a href="https://news.adobe.com/news/news-details/2023/adobe-unveils-firefly-a-family-of-new-creative-generative-ai" target="_blank">Adobe Firefly Image Model</a>
					<div class="date">April 2023</div>
					<div class="description">Started the effort that would ultimately result in the foundation of Adobe Firefly; developed and trained the core diffusion model.</div>
				</div>
			</div>
		</div>
	</div>
</section>

<!-- Publications -->
<section id="publications" class="wrapper style2 special">
	<div class="container 75%">
		<h2 style="text-align: center">Selected Publications</h2>
		<p style="text-align:center;">For a full list, have a look at my <a
				href="https://scholar.google.com/citations?user=SugTw28AAAAJ&hl" target="_blank">Google Scholar</a>
			page.</p>
		<hr />

		<ul class="row 150%">
			<ul class="3u 4u(large) 12u$(medium)">
				<div class="image rounded" style="margin-top:1cm;"><img
						src="/images/publication/thumbnails/shotadapter.jpg" width="180" alt=""
						style="border:none;" />
				</div>
			</ul>

			<ul class="9u$ 8u$(large) 12u$(medium)">
				<header>
					<h4>ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models</h4>
					<p>A framework that enables text-to-multi-shot video generation with shot-specific conditioning from a pretrained T2V model.</p>
				</header>
				<p>O. Kara, K. Singh, F. Liu, D. Ceylan, J. M. Rehg, <span style="text-decoration: underline;">T. Hinz</span>, <span
						style="font-weight: bold">Conference on Computer Vision and Pattern Recognition</span> 2025.</p>
				<ul class="btn-group btn-group-sm">
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://shotadapter.github.io/' target="_blank">Project</a>
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://arxiv.org/abs/2501.02332' target="_blank">Paper</a>
				</ul>
			</ul>
		</ul>

		<hr />

		<ul class="row 150%">
			<ul class="3u 4u(large) 12u$(medium)">
				<div class="image rounded" style="margin-top:1cm;"><img
						src="/images/publication/thumbnails/pers_residuals.jpg" width="180" alt=""
						style="border:none;" />
				</div>
			</ul>

			<ul class="9u$ 8u$(large) 12u$(medium)">
				<header>
					<h4>Personalized Residuals for Concept-Driven Text-to-Image Generation</h4>
					<p>A novel and efficient approach for enabling personalized image generation with diffusion models.
					</p>
				</header>
				<p>C. Ham, M. Fisher, J. Hays, N. Kolkin, Y. Liu, R. Zhang, <span style="text-decoration: underline;">T.
						Hinz</span>,
					<span style="font-weight: bold">Conference on Computer Vision and Pattern Recognition</span> 2024.
				</p>
				<ul class="btn-group btn-group-sm">
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://cusuh.github.io/personalized-residuals/' target="_blank">Project</a>
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://openaccess.thecvf.com/content/CVPR2024/html/Ham_Personalized_Residuals_for_Concept-Driven_Text-to-Image_Generation_CVPR_2024_paper.html'
						target="_blank">Paper</a>
				</ul>
			</ul>
		</ul>

		<hr />

		<ul class="row 150%">
			<ul class="3u 4u(large) 12u$(medium)">
				<div class="image rounded" style="margin-top:1cm;"><img src="/images/publication/thumbnails/mcm.jpg"
						width="180" alt="" style="border:none;" />
				</div>
			</ul>

			<ul class="9u$ 8u$(large) 12u$(medium)">
				<header>
					<h4>Modulating Pretrained Diffusion Models for Multimodal Image Synthesis</h4>
					<p>A multimodal conditioning module (MCM) for enabling conditional image synthesis using pretrained
						diffusion models.</p>
				</header>
				<p>C. Ham, J. Hays, J. Lu, K. Singh, Z. Zhang, <span style="text-decoration: underline;">T. Hinz</span>,
					<span style="font-weight: bold">SIGGRAPH Conference Proceedings</span> 2023.
				</p>
				<ul class="btn-group btn-group-sm">
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://mcm-diffusion.github.io/' target="_blank">Project</a>
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://arxiv.org/abs/2302.12764' target="_blank">Paper</a>
				</ul>
			</ul>
		</ul>

		<hr />

		<ul class="row 150%">
			<ul class="3u 4u(large) 12u$(medium)">
				<div class="image rounded" style="margin-top:1cm;"><img
						src="/images/publication/thumbnails/smartbrush.jpg" width="180" alt="" style="border:none;" />
				</div>
			</ul>

			<ul class="9u$ 8u$(large) 12u$(medium)">
				<header>
					<h4>SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model</h4>
					<p>A diffusion model for shape-guided inpainting with better shape control and background
						preservation within the inpainted region.</p>
				</header>
				<p>S. Xie, Z. Zhang, Z. Lin, <span style="text-decoration: underline;">T. Hinz</span>, K. Zhang, <span
						style="font-weight: bold">Conference on Computer Vision and Pattern Recognition</span> 2023.</p>
				<ul class="btn-group btn-group-sm">
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://arxiv.org/abs/2212.05034' target="_blank">Paper</a>
				</ul>
			</ul>
		</ul>

		<hr />

		<ul class="row 150%">
			<ul class="3u 4u(large) 12u$(medium)">
				<div class="image rounded" style="margin-top:1cm;"><img src="/images/publication/thumbnails/asset.jpg"
						width="180" alt="" style="border:none;" /></div>
			</ul>

			<ul class="9u$ 8u$(large) 12u$(medium)">
				<header>
					<h4>ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions</h4>
					<p>A neural architecture for automatically modifying an input high-resolution image according to a
						user's edits on its semantic segmentation map.</p>
				</header>
				<p>D. Liu, S. Shetty, <span style="text-decoration: underline;">T. Hinz</span>, M. Fisher,
					R. Zhang, T. Park, E. Kalogerakis, <span style="font-weight: bold">ACM Transactions on Graphics
						(SIGGRAPH 2022)</span> 2022.</p>
				<ul class="btn-group btn-group-sm">
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://people.cs.umass.edu/~dliu/projects/ASSET/' target="_blank">Project</a>
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://github.com/DifanLiu/ASSET' target="_blank">Code</a>
				</ul>
			</ul>
		</ul>

		<hr />

		<ul class="row 150%">
			<ul class="3u 4u(large) 12u$(medium)">
				<div class="image rounded" style="margin-top:1cm;"><img
						src="/images/publication/thumbnails/consingan.jpg" width="180" alt="" style="border:none;" />
				</div>
			</ul>

			<ul class="9u$ 8u$(large) 12u$(medium)">
				<header>
					<h4>Improved Techniques for Training Single-Image GANs</h4>
					<p>Improving the results and training speed of single-image GANs.</p>
				</header>
				<p><span style="text-decoration: underline;">T. Hinz</span>, M. Fisher, O. Wang, S. Wermter, <span
						style="font-weight: bold">IEEE Winter Conference on Applications of Computer Vision</span> 2021.
				</p>
				<ul class="btn-group btn-group-sm">
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://arxiv.org/abs/2003.11512' target="_blank">Paper</a>
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='/2020/03/24/improved-techniques-for-training-single-image-gans'>Blog Post</a>
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://github.com/tohinz/ConSinGAN' target="_blank">Code</a>
				</ul>
			</ul>
		</ul>

		<hr />

		<ul class="row 150%">
			<ul class="3u 4u(large) 12u$(medium)">
				<div class="image rounded" style="margin-top:1cm;"><img
						src="/images/publication/thumbnails/soa-for-tti.jpg" width="180" alt="" style="border:none;" />
				</div>
			</ul>

			<ul class="9u$ 8u$(large) 12u$(medium)">
				<header>
					<h4>Semantic Object Accuracy for Generative Text-to-Image Synthesis</h4>
					<p>A novel GAN architecture and an improved metric to evaluate generative text-to-image synthesis
						models.</p>
				</header>
				<p><span style="text-decoration: underline;">T. Hinz</span>, S. Heinrich, S. Wermter, <span
						style="font-weight: bold">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>
					2020.</p>
				<ul class="btn-group btn-group-sm">
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://ieeexplore.ieee.org/document/9184960' target="_blank">Paper</a>
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='/2019/10/30/semantic-object-accuracy-for-generative-text-to-image-synthesis'>Blog Post</a>
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://github.com/tohinz/semantic-object-accuracy-for-generative-text-to-image-synthesis'
						target="_blank">Code</a>
				</ul>
			</ul>
		</ul>

		<hr />

		<ul class="row 150%">
			<ul class="3u 4u(large) 12u$(medium)">
				<div class="image rounded" style="margin-top:1cm;"><img src="/images/publication/thumbnails/mogan.jpg"
						width="180" alt="" style="border:none;" /></div>
			</ul>

			<ul class="9u$ 8u$(large) 12u$(medium)">
				<header>
					<h4>Generating Multiple Objects at Spatially Distinct Locations</h4>
					<p>Fine-grained control over the placement and identity of objects in images generated with a
						Generative Adversarial Network.</p>
				</header>
				<p><span style="text-decoration: underline;">T. Hinz</span>, S. Heinrich, S. Wermter, <span
						style="font-weight: bold">International Conference on Learning Representations</span> 2019.</p>
				<ul class="btn-group btn-group-sm">
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://openreview.net/forum?id=H1edIiA9KQ' target="_blank">Paper</a>
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='/2019/01/09/generating-multiple-objects-at-spatially-distinct-locations'>Blog Post</a>
					<a role="button" style="text-decoration: none" class="btn btn-primary"
						href='https://github.com/tohinz/multiple-objects-gan' target="_blank">Code</a>
				</ul>
			</ul>
		</ul>
		<br>
		<hr />
		<a href="/publications/index" class="btn btn-primary btn-lg" role="button" style="text-decoration: none">See
			All</a>
	</div>
</section>

<!-- Blog -->
<section id="blog" class="wrapper style1 special">
	<div class="container 75%">
		{% for page in site.posts %}
		<ul class="row 150%">
			<ul class="3u 4u(large) 12u$(medium)">
				<div class="image rounded" style="margin-top:1cm;"><a href="{{ page.url }}"><img
							src="{{ page.thumbnail }}" width="180" alt="" style="border:none;" /></a></div>
			</ul>

			<ul class="9u$ 8u$(large) 12u$(medium)">
				<header>
					<h4><a href="{{ page.url }}">{{ page.title }}</a></h4>
					<p>Posted on {{ page.date | date: "%d %b %Y" }} | Reading time: {% include read_time.html %}</p>
				</header>
				<p>{{ page.description }}</p>
				<p><a class="btn btn-primary btn-sm" role="button" style="text-decoration: none"
						href="{{ page.url }}">Continue reading</a></p>
			</ul>
		</ul>
		{% if forloop.index == 4 %}
		{% break %}
		{% endif %}
		{% endfor %}
		<hr />
		<a href="/blog/index" class="btn btn-primary btn-lg" role="button" style="text-decoration: none">See All</a>
		<br>
	</div>
</section>

<!-- CV -->
<section id="cv" class="wrapper style2 special">
	<div class="container 50%">
		<h2>Curriculum Vitae</h2>
		<ul class="actions">
			<li><a href="documents/CV_Tobias_Hinz.pdf" class="btn btn-primary btn-lg" role="button"
					style="text-decoration: none" target="_blank">Download CV</a></li>
		</ul>
	</div>
</section>
