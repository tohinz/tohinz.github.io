---
layout: blog
title: Improved Techniques for Training Single-Image GANs
author: Tobias Hinz
date: 2020-03-24
description: Overview of our paper about training GANs on a single image for several different tasks.
thumbnail: /images/publication/thumbnails/consingan.jpg
mathjax: false
comments: true
---

Recently there has been an interest in the potential of learning generative models from a single image, as opposed to from a large dataset. This task is of practical significance, as it means that generative models can be used in domains where collecting a large dataset is not feasible. The first work that introduced GANs for unconditional image generation trained on a *single natural* image was [SinGAN](https://arxiv.org/abs/1905.01164){:target="_blank"}, presented at ICCV 2019. Before that, several other approaches trained GANs on single images, however, these images where either not *natural* images (e.g. [these](https://link.springer.com/chapter/10.1007/978-3-319-46487-9_43){:target="_blank"} [approaches](https://arxiv.org/abs/1705.06566){:target="_blank"} [for](https://arxiv.org/abs/1805.04487){:target="_blank"} texture synthesis) or the task was not to learn an unconditional model (e.g. [InGAN](https://arxiv.org/abs/1812.00231){:target="_blank"} for image retargeting). This blog post summarizes our paper [Improved Techniques for Training Single-Image GANs](https://www.google.com){:target="_blank"} in which we look into several mechanisms that improve the training and generation capabilities of GANs on single images. 

Our main contributions, which we will summarize in the following post, are:

- training architecture and optimization: we identify several aspects of the architecture and optimization process that are critical to the success of training Single-Image GANs;
- rescaling approach for multi-stage training on different resolutions: the approach to rescale images for the different training stages directly affects the number of stages we need to train on;
- fine-tuning approach for several tasks: we introduce a fine-tuning step on pre-trained models to obtain even better results on tasks such as image harmonization.

In the following, we will talk about each of these points in a bit more detail.



### Training Architecture and Optimization

To put our approach into perspective we quickly describe SinGAN's training procedure, before giving a more detailed overview of our approach. SinGAN trains several individual generator networks on images of increasing resolutions. Here, the first generator - working on the smallest image resolution - is the only unconditional generator that generates a image from random noise (see Fig. 1 for a visualization).

<figure><center><img src="/images/blog/ConSinGAN/models/singan/singan0.jpg" alt="SinGAN Model" width="50%"/><figcaption>Fig. 1: Training of the first generator in the SinGAN.</figcaption></center></figure>

The important part is that the discriminator is a path discriminator, i.e. the discriminator never sees the image as a whole, but only parts of it. Through this it learns what "real" image patches look like. Since the discriminator never sees the image as a whole the generator can fool it by generating images that are different from the training image on a global perspective, but similar when looking only at patch statistics.

The generators working on higher resolutions take the image generated by the previous generator as input and generate an image of the current (higher) resolution from it. All generators are trained in isolation, meaning all previous generators' weights are kept frozen when training the current generator. Fig. 2 shows a visualization of how the training proceeds throughout the different resolutions.

<figure><center><img src="/images/blog/ConSinGAN/models/singan/singan1.jpg" alt="SinGAN Model" width="50%"/><figcaption>Fig. 2: Progress of SinGAN training throughout the different resolutions. Notice how only one generator is trained at a given time, while all others are frozen.</figcaption></center></figure>

During our experiments we observe that training only one generator at a given time, as well as propagating images instead of feature maps from one generator to the next, limits the interactions between the generators. As a consequence, we train our generators end-to-end, i.e. we train more than one generator at a given time, and each generator takes as input the features (instead of images) generated by the previous generator. Since we train several generators concurrently, we call our model the *Concurrent-Single-Image-GAN" (ConSinGAN). See Fig. 3 for a visualization.

<figure><center><img src="/images/blog/ConSinGAN/models/consingan/consingan.jpg" alt="ConSinGAN Model" width="50%"/><figcaption>Fig. 3: Progress of ConSinGAN training throughout the different resolutions. Notice how we do not train only one generator and how each generator gets the features generated by the previous generator as input (instead of images as in SinGAN).</figcaption></center></figure>

Now, the problem with training all generators is that this quickly leads to overfitting. This means that the final model does not generate any "new" images, but instead only generates the training image, which is obviously not what we want. We introduce two ways of how to prevent this from happening:

1. only train *some* of the generators at any given time, not *all* of them and
2. use different learning rates for the different generators, with smaller learning rates for generators earlier generators when training a given generator.

Fig. 4 visualizes our model with these two approaches implemented. As a default, we train up to three generators concurrently and scale the learning rate by 1/10 and 1/100 respectively for the lower generators.

<figure><center><img src="/images/blog/ConSinGAN/training/consingan.jpg" alt="ConSinGAN Training" width="50%"/><figcaption>Fig. 4: Training of a ConSinGAN on six stages. In our default setting we train up to three generatos concurrently and scale the learning rates of lower generators by factors 1/10 and 1/100 respectively.</figcaption></center></figure>

One interesting aspect of this approach is that we are now able to trade-off image diversity for fidelity by changing, e.g., the way of how we scale the learning rate for lower generators. Using a larger learning rate for the lower generators means that the generated images will, on average, be more similar to the training image. Using smaller learning rates for the lower generators, on the other hand, means that we will have more diversity (of possibly worse quality) in the generated images. Fig. 5 shows an example of what this might look like and we can see that larger learning rates of lower generators results in better image fidelity but less image diversity.

<figure><center><img src="/images/blog/ConSinGAN/training/lr_scale.jpg" alt="ConSinGAN Learning Rate Scaling" width="50%"/><figcaption>Fig. 5: Effects of different learning rate scalings on training. The top row shows images generated by a model where the learning rates of the lower generators were scaled by 1/10 and 1/100 respectively. The bottorm row shows results from a model where the learning rates were scaled by 1/2 and 1/4 respectively.</figcaption></center></figure>



---
---

